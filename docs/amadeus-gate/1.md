---
title: '第 1 章：识别问题——从像素到分类'
createTime: 2025/12/29 21:00:00
permalink: /amadeus-gate/recognition/
---

# 第 1 章：识别问题——从像素到分类

> *"当机器第一次『看到』数字，感知之门打开了。"*
>
> —— 红莉栖

::: tip 故事背景
Lab 里的冈部发现了一台机器——它能读取手写数字。
> "这有什么难的？"红莉栖说，"人类天生就会识别啊。"
> 但要让机器做到这件事，比想象中困难得多......
:::

## 章节目标

通过本章学习，你将能够：

- 理解什么是分类问题，以及为什么识别数字比想象中困难
- 了解 MNIST 数据集的结构和特点
- 实现一个简单的最近邻分类器，并理解其局限性
- 理解人工特征工程的局限性，以及机器学习的核心思想
- 掌握线性分类器的原理和感知机学习算法

---

## 1.1 从像素到分类：问题的提出

### 1.1.1 为什么识别看起来很简单？

作为人类，我们几乎可以毫不费力地识别手写数字。看到一个歪歪扭扭的「7」，我们立刻就知道它是「7」；看到一个潦草的「3」，我们也能准确地将它与「8」区分开。

但仔细想想，这其实是一个非常了不起的能力：

```
人类识别数字的过程：
│
├── 视觉信号进入眼睛（光信号）
│
├── 大脑处理：
│   ├── 边缘检测（哪里有线条？）
│   ├── 形状分析（线条如何连接？）
│   ├── 特征提取（有几个闭环？线条方向如何？）
│   └── 模式匹配（与记忆中的数字模板比对）
│
└── 输出识别结果（「3」）
```

这个过程对我们来说是如此自然，以至于我们很少意识到它有多复杂。

### 1.1.2 机器眼中的「数字」

然而，对于计算机来说，数字图像只是**一堆数字**：

```
一个 28×28 的数字图像 = 784 个像素值

┌────────────────────────────────────────────┐
│  0   0   0  123  255  200   50    0   ...  │
│  0   0  50  200  255  180   80    0   ...  │
│  0  30 150  255  255  200   90    0   ...  │
│  ...                                     │
└────────────────────────────────────────────┘
         ↑
    每个像素值：0-255（灰度值）
```

**核心问题：** 如何从这 784 个数字中，判断出它是 0-9 中的哪个数字？

::: tip 思考
如果让你写代码来实现这个功能，你会怎么做？
:::

### 1.1.3 分类问题的形式化

在机器学习中，这种问题被称为**分类问题**：

```
输入：x（784 个像素值组成的向量）
    ↓
函数 f(x)
    ↓
输出：y（0-9 中的一个，即类别标签）
```

**分类问题的本质：** 找到一个函数，将输入映射到离散的类别。

---

## 1.2 MNIST：我们的第一个「琥珀」

> *"这是我们遇到的第一个『琥珀』——里面保存着数字世界的记忆。"*
>
> —— 冈部

### 1.2.1 数据集概述

MNIST（Modified National Institute of Standards and Technology）是机器学习领域最经典的数据集之一，被称为**深度学习的「Hello World」**。

| 属性 | 说明 |
|------|------|
| **规模** | 70,000 张图像（60,000 张训练 + 10,000 张测试） |
| **尺寸** | 28×28 像素 |
| **内容** | 0-9 的手写数字 |
| **格式** | 每个像素值 0-255（灰度值） |
| **标签** | 每个图像对应一个数字类别（0-9） |

### 1.2.2 数据可视化

让我们来看看 MNIST 中的数字是什么样子：

```
数字「0」的各种写法：

  ████        ████        ████        ████
 █    █      █    █      █    █      █    █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
 █    █      █    █      █    █      █    █
  ████        ████        ████        ████

观察：每个人的「0」写法都不一样，但人类依然能准确识别
```

::: important 观察与思考
观察 MNIST 数据集，思考以下问题：

1. 同一个数字有多少种不同的写法？
2. 不同数字之间有哪些相似的写法（比如 3 和 8）？
3. 机器如何区分这些相似的数字？
:::

### 1.2.3 数据预处理

在实际使用 MNIST 之前，我们需要进行一些预处理：

```python
# 数据预处理示例
import numpy as np

# 1. 像素值归一化：将 0-255 缩放到 0-1
images = images / 255.0

# 2. 标签编码：将数字标签转换为 one-hot 编码
# 例如：数字 3 → [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]

# 3. 数据形状调整：(N, 28, 28) → (N, 784)
images = images.reshape(-1, 28 * 28)
```

---

## 1.3 第一个分类器：最近邻

### 1.3.1 直观思路

让我们从最简单的方法开始：**最近邻分类器（K-Nearest Neighbors, KNN）**。

**核心思想：** "长得像，就是一类"

```
测试图像
    │
    ▼
┌─────────────────────────────┐
│  计算与所有训练图像的距离     │
│                             │
│  距离：d₁, d₂, d₃, ..., dₙ  │
│       ↓                     │
│  找到最近的 k 个邻居         │
│       ↓                     │
│  多数投票决定类别            │
└─────────────────────────────┘
```

### 1.3.2 距离计算

最常用的距离是**欧几里得距离**：

```python
import numpy as np

def euclidean_distance(img1, img2):
    """计算两张图像的欧几里得距离"""
    return np.sqrt(np.sum((img1 - img2) ** 2))

def predict_knn(test_image, train_images, train_labels, k=3):
    """
    KNN 预测函数

    参数：
        test_image: 测试图像
        train_images: 训练图像集
        train_labels: 训练标签集
        k: 近邻数量
    """
    # 1. 计算与所有训练图像的距离
    distances = [euclidean_distance(test_image, img)
                 for img in train_images]

    # 2. 找到距离最小的 k 个邻居
    k_nearest_indices = np.argsort(distances)[:k]
    k_nearest_labels = train_labels[k_nearest_indices]

    # 3. 多数投票
    from collections import Counter
    predicted_label = Counter(k_nearest_labels).most_common(1)[0][0]

    return predicted_label
```

### 1.3.3 代码实现

```python
import numpy as np
from collections import Counter

class NearestNeighbor:
    """最近邻分类器"""

    def __init__(self):
        self.train_images = None
        self.train_labels = None

    def train(self, images, labels):
        """训练：只是简单地存储所有训练数据"""
        self.train_images = images
        self.train_labels = labels

    def predict(self, test_image, k=1):
        """预测：找到最近的邻居"""
        # 计算所有距离
        distances = np.linalg.norm(
            self.train_images - test_image,
            axis=1
        )

        # 找到最近邻
        nearest_idx = np.argmin(distances)
        return self.train_labels[nearest_idx]
```

### 1.3.4 实验观察

::: important 实验任务
运行以下实验，观察 KNN 在 MNIST 上的表现：

1. 使用不同的 k 值（1, 3, 5, 7）测试准确率
2. 可视化错误分类的样本，分析失败原因
3. 思考：KNN 的主要问题是什么？
:::

**KNN 的问题：**

| 问题 | 说明 | 后果 |
|------|------|------|
| **维度灾难** | 高维空间中距离失去意义 | 所有点都差不多「远」 |
| **计算昂贵** | 预测时需要比较所有训练样本 | 70,000 个样本，每次比较都耗时 |
| **存储需求** | 需要存储所有训练数据 | 内存占用大 |

::: tip 维度灾难
想象一下：在二维平面上，你可以在单位正方形内放很多点，每个点都有自己的「邻居」。但在 784 维的空间中，情况完全不同——数据变得极其稀疏，大多数点之间的距离都差不多。

这就是为什么 KNN 在高维数据上表现不佳。
:::

---

## 1.4 特征工程 vs 特征学习

### 1.4.1 人工特征工程

在深度学习兴起之前，机器学习主要依赖**人工设计的特征**：

**传统方法：** 人工提取特征 → 使用分类器

```
原始图像
    │
    ▼
┌─────────────────────────────────┐
│  特征提取（人工设计）            │
│                                 │
│  • 笔画方向直方图                │
│  • 闭环数量（有几个洞？）        │
│  • 笔画交叉点数量                │
│  • 整体形状比例                  │
└─────────────────────────────────┘
    │
    ▼
特征向量（人工设计的描述子）
    │
    ▼
传统分类器（SVM、决策树等）
```

**特征示例：**

| 数字 | 闭环数 | 笔画方向 | 交叉点数 |
|------|--------|----------|----------|
| 0 | 1 | 水平+垂直 | 0 |
| 1 | 0 | 垂直 | 0 |
| 2 | 0 | 水平+斜线 | 1 |
| 3 | 0 | 曲线 | 0 |
| 4 | 0 | 多方向 | 2 |
| 5 | 0 | 曲线+水平 | 1 |
| 6 | 1 | 曲线 | 1 |
| 7 | 0 | 水平+斜线 | 1 |
| 8 | 2 | 曲线 | 0 |
| 9 | 1 | 曲线 | 1 |

### 1.4.2 人工特征的局限性

**问题 1：规则永远赶不上数据变化**

```
「3」的不同写法：
  正常    手写    艺术    潦草
  ━     ━      ㄟ      𝟛

人工规则无法覆盖所有情况！
```

**问题 2：特征设计需要专业知识**

- 需要领域专家设计特征
- 不同任务需要不同特征
- 特征设计耗时耗力

### 1.4.3 特征学习的革命

**核心思想：** 与其告诉机器「规则」，不如让机器自己「学习」规则。

```
原始图像
    │
    ▼
┌─────────────────────────────────┐
│  神经网络                       │
│                                 │
│  自动学习有用的特征表示          │
│  （从数据中自主发现规律）        │
└─────────────────────────────────┘
    │
    ▼
学习到的特征表示
    │
    ▼
分类层
```

**学习到的特征 vs 人工特征：**

| 对比维度 | 人工特征 | 特征学习 |
|----------|----------|----------|
| 设计者 | 人类专家 | 自动学习 |
| 泛化能力 | 依赖设计质量 | 从数据中泛化 |
| 适应性 | 任务特定 | 任务通用 |
| 可解释性 | 通常可解释 | 难以解释 |

::: tip 核心启示
**让数据自己说话。**

这就是机器学习的核心思想：不是我们告诉机器怎么做，而是让机器从数据中发现规律。
:::

---

## 1.5 线性分类器：画一条线

### 1.5.1 几何直觉

让我们从最简单的分类器开始：**线性分类器**。

**核心思想：** 在高维空间中，画一个「分界面」来区分不同类别。

```
二维空间中的分类（简化示意）：

        ····●····●····
        ··○·○···○·○···
        ·○···○··○···○·
        ──────────────── 分界面
        ·○···○··○···○·
        ··○·○···○·○···
        ····○····○····

· = 类别 A 的点
○ = 类别 B 的点
```

**推广到高维：**

- 图像 → 高维空间（784 维）中的一个点
- 分类 → 画一个「超平面」来分隔不同类别

### 1.5.2 数学表达

线性分类器的数学形式：

```
f(x) = w · x + b
output = sign(f(x))
```

| 符号 | 含义 |
|------|------|
| x | 输入向量（784 个像素值） |
| w | 权重向量（决定分类边界的位置和方向） |
| b | 偏置（决定分类边界的位置） |
| · | 点积运算 |
| sign() | 符号函数（大于 0 输出 1，否则输出 -1） |

**直观理解：**

- **w（权重）：** 决定分类边界朝哪个方向倾斜
- **b（偏置）：** 决定分类边界距离原点的位置

### 1.5.3 感知机学习算法

**感知机（Perceptron）** 是最简单的线性分类器，由 Frank Rosenblatt 于 1957 年提出。

```python
class Perceptron:
    """感知机分类器"""

    def __init__(self, learning_rate=0.1):
        self.lr = learning_rate
        self.weights = None
        self.bias = 0

    def fit(self, X, y, epochs=100):
        """
        训练感知机

        参数：
            X: 训练数据 (n_samples, n_features)
            y: 标签 (n_samples,)
            epochs: 训练轮数
        """
        n_samples, n_features = X.shape

        # 初始化权重
        self.weights = np.zeros(n_features)
        self.bias = 0

        # 感知机学习算法
        for epoch in range(epochs):
            for i in range(n_samples):
                x_i = X[i]
                y_i = y[i]

                # 计算预测值
                linear_output = np.dot(x_i, self.weights) + self.bias
                y_pred = np.sign(linear_output)

                # 如果预测错误，更新权重
                if y_pred != y_i:
                    self.weights += self.lr * y_i * x_i
                    self.bias += self.lr * y_i

    def predict(self, X):
        """预测新样本"""
        linear_output = np.dot(X, self.weights) + self.bias
        return np.sign(linear_output)
```

**更新规则：**

```
如果分类正确：不更新
如果分类错误：
    w = w + η · y · x
    b = b + η · y
```

| 符号 | 含义 |
|------|------|
| η（eta） | 学习率（控制更新步长） |
| y | 真实标签（+1 或 -1） |
| x | 输入样本 |

::: tip 直观理解
感知机学习就像调整一个「分界面」：

1. 找到错误分类的样本
2. 把分界面往正确的方向「推」一点
3. 重复直到所有样本都分类正确

这就像蒙着眼睛调椅子：坐上去试试（预测），不舒服就挪一下（更新），直到坐得舒服为止。
:::

### 1.5.4 线性分类器的局限

**问题：异或（XOR）不可分**

```
异或问题：
  输入 A  输入 B  输出
    0      0      0
    0      1      1
    1      0      1
    1      1      0

可视化：

    1 │  ○      ●
      │
    0 │  ●      ○
      └─────────────
        0      1

● = 输出 0
○ = 输出 1

无法用一条直线分开！
```

线性分类器无法解决 XOR 问题，这是它最大的局限。

**解决思路：** 使用多层神经网络，引入非线性。

::: important 思考
1. 线性分类器为什么无法解决 XOR 问题？
2. 如果我们叠加多个线性分类器，会发生什么？
3. 什么操作可以打破「线性」的局限？
:::

---

## 1.6 章节总结

### 核心概念回顾

| 概念 | 说明 |
|------|------|
| **分类问题** | 将输入映射到离散类别的监督学习问题 |
| **MNIST** | 手写数字数据集，28×28 灰度图像 |
| **KNN** | 最近邻分类器，基于距离的简单方法 |
| **维度灾难** | 高维空间中距离失去意义的现象 |
| **特征工程** | 人工设计特征表示 |
| **特征学习** | 从数据中自动学习特征表示 |
| **线性分类器** | 使用超平面进行分类的模型 |
| **感知机** | 最简单的线性分类器，学习算法 |

### 章节要点

1. **识别数字是复杂的**：人类看似轻松的识别能力，其实涉及复杂的视觉处理过程
2. **KNN 简单但有限**：最近邻分类器直观易懂，但存在维度灾难和计算效率问题
3. **特征工程 vs 特征学习**：人工特征设计耗时且难以泛化，自动特征学习是更好的选择
4. **线性分类器有局限**：单层线性分类器无法解决 XOR 问题

### 本章任务

::: important 本章任务清单
- [ ] 运行 KNN 代码，观察不同 k 值的效果
- [ ] 可视化 MNIST 数据，直观感受数字的多样性
- [ ] 实现感知机算法，理解其学习过程
- [ ] 思考：如何解决线性分类器无法处理 XOR 的问题？
- [ ] 扩展阅读：了解感知机的历史和局限
:::

### 预告

下一章，我们将学习**多层感知机（MLP）**，通过引入隐藏层和非线性激活函数，解决 XOR 问题，并向真正的神经网络迈进。

> *"单层的局限，需要多层来突破。"*
>
> —— 红莉栖
