---
title: '第 1 章：识别问题——从像素到分类'
createTime: 2025/12/31 13:00:00
permalink: /amadeus-gate/recognition/
---

# 第 1 章：识别问题——从像素到分类

> *"当机器第一次『看到』数字，感知之门打开了。"*
>
> —— 红莉栖

::: tip 故事背景
Lab 里的冈部发现了一台机器——它能读取手写数字。
> "这有什么难的？"红莉栖说，"人类天生就会识别啊。"
> 但要让机器做到这件事，比想象中困难得多......
> 冈部决定从最简单的分类器开始，一步步揭开机器学习的神秘面纱。
:::

---

## 1.1 从像素到分类：问题的提出

### 1.1.1 为什么识别看起来很简单？

作为人类，我们几乎可以毫不费力地识别手写数字。看到一个歪歪扭扭的「7」，我们立刻就知道它是「7」；看到一个潦草的「3」，我们也能准确地将它与「8」区分开。

但仔细想想，这其实是一个非常了不起的能力：

```
人类识别数字的过程：
│
├── 视觉信号进入眼睛（光信号）
│
├── 大脑处理：
│   ├── 边缘检测（哪里有线条？）
│   ├── 形状分析（线条如何连接？）
│   ├── 特征提取（有几个闭环？线条方向如何？）
│   └── 模式匹配（与记忆中的数字模板比对）
│
└── 输出识别结果（「3」）
```

这个过程对我们来说是如此自然，以至于我们很少意识到它有多复杂。

### 1.1.2 机器眼中的「数字」

然而，对于计算机来说，数字图像只是**一堆数字**：

```
一个 28×28 的数字图像 = 784 个像素值

┌────────────────────────────────────────────┐
│  0   0   0  123  255  200   50    0   ...  │
│  0   0  50  200  255  180   80    0   ...  │
│  0  30 150  255  255  200   90    0   ...  │
│  ...                                     │
└────────────────────────────────────────────┘
         ↑
    每个像素值：0-255（灰度值）
```

**核心问题：** 如何从这 784 个数字中，判断出它是 0-9 中的哪个数字？

::: tip 思考
如果让你写代码来实现这个功能，你会怎么做？
:::

### 1.1.3 分类问题的形式化

在机器学习中，这种问题被称为**分类问题**：

```mermaid
graph LR
    A["输入：x<br/>784 个像素值组成的向量"] --> B["函数 f(x)"]
    B --> C["输出：y<br/>0-9 中的一个，即类别标签"]
```

**分类问题的本质：** 找到一个函数，将输入映射到离散的类别。

::: info 数学知识
**向量（Vector）**

**定义：** 向量是 n 个数组成的有序序列，可以表示为列向量或行向量。

**公式：**
$$
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \in \mathbb{R}^n
$$

**直观理解：** 在几何上，向量可以看作空间中的一个点或有方向的线段。784 维向量可以看作 784 维空间中的一个点。

**在 AI 中的应用：** 图像可以展平为向量（如 MNIST 的 28×28 图像 → 784 维向量），所有机器学习算法都在向量空间中进行运算。
:::

---

## 1.2 MNIST：我们的第一个「琥珀」

> *"这是我们遇到的第一个『琥珀』——里面保存着数字世界的记忆。"*
>
> —— 冈部

### 1.2.1 数据集概述

MNIST（Modified National Institute of Standards and Technology）是机器学习领域最经典的数据集之一，被称为**深度学习的「Hello World」**。

| 属性 | 说明 |
|------|------|
| **规模** | 70,000 张图像（60,000 张训练 + 10,000 张测试） |
| **尺寸** | 28×28 像素 |
| **内容** | 0-9 的手写数字 |
| **格式** | 每个像素值 0-255（灰度值） |
| **标签** | 每个图像对应一个数字类别（0-9） |

### 1.2.2 数据可视化

让我们来看看 MNIST 中的数字是什么样子：

```
数字「0」的各种写法：

  ████        ████        ████        ████
 █    █      █    █      █    █      █    █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
█      █    █      █    █      █    █      █
 █    █      █    █      █    █      █    █
  ████        ████        ████        ████

观察：每个人的「0」写法都不一样，但人类依然能准确识别
```

::: important 观察与思考
观察 MNIST 数据集，思考以下问题：

1. 同一个数字有多少种不同的写法？
2. 不同数字之间有哪些相似的写法（比如 3 和 8）？
3. 机器如何区分这些相似的数字？
:::

### 1.2.3 数据预处理

在实际使用 MNIST 之前，我们需要进行一些预处理：

```python
import numpy as np

# 1. 像素值归一化：将 0-255 缩放到 0-1
images = images / 255.0

# 2. 标签编码：将数字标签转换为 one-hot 编码
# 例如：数字 3 → [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]

# 3. 数据形状调整：(N, 28, 28) → (N, 784)
images = images.reshape(-1, 28 * 28)
```

::: info 数学知识
**矩阵（Matrix）**

**定义：** 矩阵是 m×n 个数组成的二维数组，可以看作向量的集合。

**公式：**
$$
\mathbf{A} = \begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix} \in \mathbb{R}^{m \times n}
$$

**矩阵的形状：** A 的形状是 m×n（m 行 n 列）。

**在 AI 中的应用：**
- 批量图像可以表示为矩阵（每行是一张图像）
- 神经网络的权重是矩阵
- 矩阵乘法是神经网络计算的核心
:::

---

## 1.3 第一个分类器：最近邻

### 1.3.1 直观思路

让我们从最简单的方法开始：**最近邻分类器（K-Nearest Neighbors, KNN）**。

**核心思想：** "长得像，就是一类"

```
测试图像
    │
    ▼
┌─────────────────────────────┐
│  计算与所有训练图像的距离     │
│                             │
│  距离：d₁, d₂, d₃, ..., dₙ  │
│       ↓                     │
│  找到最近的 k 个邻居         │
│       ↓                     │
│  多数投票决定类别            │
└─────────────────────────────┘
```

### 1.3.2 距离计算

最常用的距离是**欧几里得距离**：

```python
import numpy as np

def euclidean_distance(img1, img2):
    """计算两张图像的欧几里得距离"""
    return np.sqrt(np.sum((img1 - img2) ** 2))

def predict_knn(test_image, train_images, train_labels, k=3):
    """
    KNN 预测函数

    参数：
        test_image: 测试图像
        train_images: 训练图像集
        train_labels: 训练标签集
        k: 近邻数量
    """
    # 1. 计算与所有训练图像的距离
    distances = [euclidean_distance(test_image, img)
                 for img in train_images]

    # 2. 找到距离最小的 k 个邻居
    k_nearest_indices = np.argsort(distances)[:k]
    k_nearest_labels = train_labels[k_nearest_indices]

    # 3. 多数投票
    from collections import Counter
    predicted_label = Counter(k_nearest_labels).most_common(1)[0][0]

    return predicted_label
```

### 1.3.3 代码实现

```python
import numpy as np
from collections import Counter

class NearestNeighbor:
    """最近邻分类器"""

    def __init__(self):
        self.train_images = None
        self.train_labels = None

    def train(self, images, labels):
        """训练：只是简单地存储所有训练数据"""
        self.train_images = images
        self.train_labels = labels

    def predict(self, test_image, k=1):
        """预测：找到最近的邻居"""
        # 计算所有距离
        distances = np.linalg.norm(
            self.train_images - test_image,
            axis=1
        )

        # 找到最近邻
        nearest_idx = np.argmin(distances)
        return self.train_labels[nearest_idx]
```

### 1.3.4 实验观察

::: important 实验任务
运行以下实验，观察 KNN 在 MNIST 上的表现：

1. 使用不同的 k 值（1, 3, 5, 7）测试准确率
2. 可视化错误分类的样本，分析失败原因
3. 思考：KNN 的主要问题是什么？
:::

**KNN 的问题：**

| 问题 | 说明 | 后果 |
|------|------|------|
| **维度灾难** | 高维空间中距离失去意义 | 所有点都差不多「远」 |
| **计算昂贵** | 预测时需要比较所有训练样本 | 70,000 个样本，每次比较都耗时 |
| **存储需求** | 需要存储所有训练数据 | 内存占用大 |

::: info 数学知识
**L2 范数（欧几里得距离）**

**定义：** 向量各分量平方和的平方根，衡量向量的"长度"。

**公式：**
$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^{n} x_i^2} = \sqrt{\mathbf{x} \cdot \mathbf{x}}
$$

**两点之间的距离：**
$$
d(\mathbf{a}, \mathbf{b}) = \|\mathbf{a} - \mathbf{b}\|_2 = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2}
$$

**直观理解：** 在二维空间中，两点之间的直线距离。

**在 AI 中的应用：** KNN 分类器的核心度量、神经网络中的正则化（L2 Regularization）。

**其他范数：**
- L1 范数（曼哈顿距离）：$\|\mathbf{x}\|_1 = \sum_{i=1}^{n} |x_i|$
- L∞ 范数：$\|\mathbf{x}\|_\infty = \max_i |x_i|$
:::

::: tip 维度灾难
想象一下：在二维平面上，你可以在单位正方形内放很多点，每个点都有自己的「邻居」。但在 784 维的空间中，情况完全不同——数据变得极其稀疏，大多数点之间的距离都差不多。

这就是为什么 KNN 在高维数据上表现不佳。
:::

---

## 1.4 特征工程 vs 特征学习

### 1.4.1 人工特征工程

在深度学习兴起之前，机器学习主要依赖**人工设计的特征**：

**传统方法：** 人工提取特征 → 使用分类器

```
原始图像
    │
    ▼
┌─────────────────────────────────┐
│  特征提取（人工设计）            │
│                                 │
│  • 笔画方向直方图                │
│  • 闭环数量（有几个洞？）        │
│  • 笔画交叉点数量                │
│  • 整体形状比例                  │
└─────────────────────────────────┘
    │
    ▼
特征向量（人工设计的描述子）
    │
    ▼
传统分类器（SVM、决策树等）
```

**特征示例：**

| 数字 | 闭环数 | 笔画方向 | 交叉点数 |
|------|--------|----------|----------|
| 0 | 1 | 水平+垂直 | 0 |
| 1 | 0 | 垂直 | 0 |
| 2 | 0 | 水平+斜线 | 1 |
| 3 | 0 | 曲线 | 0 |
| 4 | 0 | 多方向 | 2 |
| 5 | 0 | 曲线+水平 | 1 |
| 6 | 1 | 曲线 | 1 |
| 7 | 0 | 水平+斜线 | 1 |
| 8 | 2 | 曲线 | 0 |
| 9 | 1 | 曲线 | 1 |

### 1.4.2 人工特征的局限性

**问题 1：规则永远赶不上数据变化**

```
「3」的不同写法：
  正常    手写    艺术    潦草
  ━     ━      ㄟ      𝟛

人工规则无法覆盖所有情况！
```

**问题 2：特征设计需要专业知识**

- 需要领域专家设计特征
- 不同任务需要不同特征
- 特征设计耗时耗力

### 1.4.3 特征学习的革命

**核心思想：** 与其告诉机器「规则」，不如让机器自己「学习」规则。

```
原始图像
    │
    ▼
┌─────────────────────────────────┐
│  神经网络                       │
│                                 │
│  自动学习有用的特征表示          │
│  （从数据中自主发现规律）        │
└─────────────────────────────────┘
    │
    ▼
学习到的特征表示
    │
    ▼
分类层
```

**学习到的特征 vs 人工特征：**

| 对比维度 | 人工特征 | 特征学习 |
|----------|----------|----------|
| 设计者 | 人类专家 | 自动学习 |
| 泛化能力 | 依赖设计质量 | 从数据中泛化 |
| 适应性 | 任务特定 | 任务通用 |
| 可解释性 | 通常可解释 | 难以解释 |

::: tip 核心启示
**让数据自己说话。**

这就是机器学习的核心思想：不是我们告诉机器怎么做，而是让机器从数据中发现规律。
:::

---

## 1.5 线性分类器：画一条线

### 1.5.1 几何直觉

让我们从最简单的分类器开始：**线性分类器**。

**核心思想：** 在高维空间中，画一个「分界面」来区分不同类别。

```
二维空间中的分类（简化示意）：

        ····●····●····
        ··○·○···○·○···
        ·○···○··○···○·
        ──────────────── 分界面
        ·○···○··○···○·
        ··○·○···○·○···
        ····○····○···∶

· = 类别 A 的点
○ = 类别 B 的点
```

**推广到高维：**

- 图像 → 高维空间（784 维）中的一个点
- 分类 → 画一个「超平面」来分隔不同类别

### 1.5.2 数学表达

线性分类器的数学形式：

$$
f(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b
$$

| 符号 | 含义 |
|------|------|
| $\mathbf{x}$ | 输入向量（784 个像素值） |
| $\mathbf{w}$ | 权重向量（决定分类边界的位置和方向） |
| $b$ | 偏置（决定分类边界的位置） |
| $\cdot$ | 点积运算 |
| $\text{sign}()$ | 符号函数（大于 0 输出 +1，否则输出 -1） |

**直观理解：**

- **$\mathbf{w}$（权重）：** 决定分类边界朝哪个方向倾斜
- **$b$（偏置）：** 决定分类边界距离原点的位置

::: info 数学知识
**点积（Dot Product）**

**定义：** 两个向量对应分量相乘后求和。

**公式：**
$$
\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i = \|\mathbf{a}\| \|\mathbf{b}\| \cos\theta
$$

**几何意义：**
- 点积可以看作 $\mathbf{a}$ 在 $\mathbf{b}$ 方向上的投影长度乘以 $\mathbf{b}$ 的长度
- 如果 $\cos\theta > 0$（夹角小于 90°），点积为正
- 如果 $\cos\theta < 0$（夹角大于 90°），点积为负

**线性分类器的直觉：** $\mathbf{w} \cdot \mathbf{x}$ 衡量输入 $\mathbf{x}$ 与权重 $\mathbf{w}$ 的"相似程度"。如果相似度大于某个阈值（由 $b$ 决定），则分类为正类。

**在 AI 中的应用：** 线性变换、相似度度量、注意力机制的基础。
:::

### 1.5.3 感知机学习算法

**感知机（Perceptron）** 是最简单的线性分类器，由 Frank Rosenblatt 于 1957 年提出。

```python
class Perceptron:
    """感知机分类器"""

    def __init__(self, learning_rate=0.1):
        self.lr = learning_rate
        self.weights = None
        self.bias = 0

    def fit(self, X, y, epochs=100):
        """
        训练感知机

        参数：
            X: 训练数据 (n_samples, n_features)
            y: 标签 (n_samples,)
            epochs: 训练轮数
        """
        n_samples, n_features = X.shape

        # 初始化权重
        self.weights = np.zeros(n_features)
        self.bias = 0

        # 感知机学习算法
        for epoch in range(epochs):
            for i in range(n_samples):
                x_i = X[i]
                y_i = y[i]

                # 计算预测值
                linear_output = np.dot(x_i, self.weights) + self.bias
                y_pred = np.sign(linear_output)

                # 如果预测错误，更新权重
                if y_pred != y_i:
                    self.weights += self.lr * y_i * x_i
                    self.bias += self.lr * y_i

    def predict(self, X):
        """预测新样本"""
        linear_output = np.dot(X, self.weights) + self.bias
        return np.sign(linear_output)
```

**更新规则：**

```
如果分类正确：不更新
如果分类错误：
    w = w + η · y · x
    b = b + η · y
```

| 符号 | 含义 |
|------|------|
| $\eta$（eta） | 学习率（控制更新步长） |
| $y$ | 真实标签（+1 或 -1） |
| $\mathbf{x}$ | 输入样本 |

::: info 数学知识
**线性变换（Linear Transformation）**

**定义：** 线性变换是满足以下条件的变换：
1. $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$（加法保持）
2. $T(c\mathbf{u}) = cT(\mathbf{u})$（数乘保持）

**矩阵形式：** 任何线性变换都可以表示为矩阵乘法
$$
\mathbf{y} = \mathbf{W}\mathbf{x}
$$

**几何解释：** 线性变换可以对向量进行旋转、缩放、投影等操作，但不能进行平移。

**在 AI 中的应用：** 神经网络中的全连接层本质上是线性变换 followed by 非线性激活。
:::

::: tip 直观理解
感知机学习就像调整一个「分界面」：

1. 找到错误分类的样本
2. 把分界面往正确的方向「推」一点
3. 重复直到所有样本都分类正确

这就像蒙着眼睛调椅子：坐上去试试（预测），不舒服就挪一下（更新），直到坐得舒服为止。
:::

### 1.5.4 线性分类器的局限

**问题：异或（XOR）不可分**

```
异或问题：
  输入 A  输入 B  输出
    0      0      0
    0      1      1
    1      0      1
    1      1      0

可视化：

    1 │  ○      ●
      │
    0 │  ●      ○
      └─────────────
        0      1

● = 输出 0
○ = 输出 1

无法用一条直线分开！
```

线性分类器无法解决 XOR 问题，这是它最大的局限。

**解决思路：** 使用多层神经网络，引入非线性。

::: important 思考
1. 线性分类器为什么无法解决 XOR 问题？
2. 如果我们叠加多个线性分类器，会发生什么？
3. 什么操作可以打破「线性」的局限？
:::

---

## 1.6 优化基础：让机器学会学习

### 1.6.1 损失函数：衡量分类的好坏

**核心问题：** 如何衡量分类器的性能？

**损失函数（Loss Function）** 量化了模型预测与真实标签之间的差距：

::: info 数学知识
**均方误差（Mean Squared Error, MSE）**

**定义：** 预测值与真实值之差的平方的平均值。

**公式：**
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

**直观理解：** 想象每个预测误差都是一个"损失"，MSE 把所有损失平方后取平均。平方操作会让大误差受到更大的惩罚。

**在 AI 中的应用：** 回归问题的标准损失函数。
:::

::: info 数学知识
**交叉熵损失（Cross-Entropy Loss）**

**定义：** 用于分类问题，衡量预测概率分布与真实分布的差异。

**公式（二分类）：**
$$
\text{BCE} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]
$$

**公式（多分类，Softmax）：**
$$
\text{CE} = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)
$$

其中 $y_i$ 是真实标签（one-hot），$\hat{y}_i$ 是预测概率。

**直观理解：** 交叉熵衡量的是" surprise "——如果预测概率低但事件发生了，我们会非常"惊讶"（损失很大）。

**在 AI 中的应用：** 分类问题的标准损失函数，配合 Softmax 使用。
:::

### 1.6.2 梯度下降：找到最优参数

**核心思想：** 沿着损失函数下降最快的方向移动。

::: info 数学知识
**导数（Derivative）**

**定义：** 导数衡量函数在某一点的变化率。

**公式：**
$$
f'(x) = \lim_{\Delta x \to 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}
$$

**几何意义：** 导数是函数曲线在该点的切线斜率。

**在 AI 中的应用：** 梯度是导数在多变量情况下的推广，指导参数更新的方向。
:::

::: info 数学知识
**梯度（Gradient）**

**定义：** 梯度是导数在多变量函数中的推广，是一个向量，包含所有偏导数。

**公式：**
$$
\nabla_\mathbf{x} f = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
$$

**关键性质：**
- 梯度指向函数增长最快的方向
- 梯度的反方向指向函数下降最快的方向

**在 AI 中的应用：** 梯度下降算法使用负梯度方向来更新参数，使损失函数最小化。
:::

**梯度下降算法：**

```python
def gradient_descent(grad_func, initial_params, learning_rate, num_iterations):
    """
    梯度下降优化算法

    参数：
        grad_func: 计算梯度的函数
        initial_params: 初始参数
        learning_rate: 学习率
        num_iterations: 迭代次数
    """
    params = initial_params

    for i in range(num_iterations):
        # 计算梯度
        gradients = grad_func(params)

        # 更新参数（沿负梯度方向）
        params = params - learning_rate * gradients

    return params
```

**更新规则：**

$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla_{\mathbf{w}} L(\mathbf{w})
$$

| 符号 | 含义 |
|------|------|
| $\mathbf{w}$ | 模型参数（权重） |
| $\eta$ | 学习率 |
| $\nabla_{\mathbf{w}} L$ | 损失函数对权重的梯度 |
| $\leftarrow$ | 赋值操作 |

```
梯度下降可视化：

        ████ 局部最大值
       █
      █
     █      ┌─── 梯度方向（上升）
    █       │
   █        ▼
  █     ▄▄▄▄▄▄▄▄  损失函数曲面
 █    ▄
▄▄▄▄▄      └─── 负梯度方向（下降）
            参数更新方向
```

::: tip 直观理解
梯度下降就像滚下山坡：

1. 站在当前位置（当前参数）
2. 环顾四周，找到最陡峭的下坡方向
3. 迈出一小步（学习率控制步长）
4. 重复直到到达山谷（损失最小）

学习率太小 → 下山太慢
学习率太大 → 可能跳过山谷，甚至上山
:::

### 1.6.3 链式法则：反向传播的基础

::: info 数学知识
**链式法则（Chain Rule）**

**定义：** 链式法则用于计算复合函数的导数。

**单变量形式：**
$$
\frac{d}{dx}[f(g(x))] = f'(g(x)) \cdot g'(x)
$$

**多变量形式：**
$$
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}
$$

**直观理解：** 如果 y 依赖 x，z 依赖 y，那么 z 对 x 的变化率等于 z 对 y 的变化率乘以 y 对 x 的变化率。

**在 AI 中的应用：** 反向传播算法使用链式法则计算损失函数对每个参数的梯度。
:::

**链式法则在神经网络中的应用：**

假设我们有一个三层网络：
$$
\mathbf{h}_1 = \sigma(\mathbf{W}_1 \mathbf{x}), \quad
\mathbf{h}_2 = \sigma(\mathbf{W}_2 \mathbf{h}_1), \quad
\mathbf{y} = \mathbf{W}_3 \mathbf{h}_2
$$

要计算 $\frac{\partial L}{\partial \mathbf{W}_1}$，我们需要：
$$
\frac{\partial L}{\partial \mathbf{W}_1} = \frac{\partial L}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{h}_2} \cdot \frac{\partial \mathbf{h}_2}{\partial \mathbf{h}_1} \cdot \frac{\partial \mathbf{h}_1}{\partial \mathbf{W}_1}
$$

这就是**反向传播**的核心原理。

---

## 1.7 激活函数：引入非线性

### 1.7.1 为什么需要激活函数？

**问题：** 多个线性变换的组合仍然是线性变换

$$
\mathbf{y} = \mathbf{W}_3(\mathbf{W}_2(\mathbf{W}_1\mathbf{x})) = (\mathbf{W}_3\mathbf{W}_2\mathbf{W}_1)\mathbf{x} = \mathbf{W}_{\text{combined}}\mathbf{x}
$$

**解决：** 在每一层后加入非线性激活函数

```
线性变换 → 激活函数 → 非线性 → 下一层
   Wx          σ()
```

### 1.7.2 常见激活函数

::: info 数学知识
**Sigmoid 函数**

**公式：**
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**导数：**
$$
\sigma'(x) = \sigma(x)(1 - \sigma(x))
$$

**特点：**
- 输出范围：(0, 1)
- 将任意实数映射到概率区间
- 梯度在两端接近 0（梯度消失问题）

**在 AI 中的应用：** 二分类问题的输出层，早期神经网络的隐藏层。
:::

::: info 数学知识
**ReLU 函数（Rectified Linear Unit）**

**公式：**
$$
\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}
$$

**导数：**
$$
\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}
$$

**特点：**
- 计算简单（只需比较和取 max）
- 正区间梯度恒为 1（缓解梯度消失）
- 稀疏激活（输出为 0 的神经元不参与计算）

**在 AI 中的应用：** 现代神经网络的标准激活函数。
:::

```
Sigmoid vs ReLU：

Sigmoid:                    ReLU:
    ┌──────┐                    ┌───┐
    │   ╲  │                    │╲  │
────┘    └───┘                ────┘   └─────
   -3    0    3                    0      x

梯度饱和（两端趋近0）        梯度恒为1（正区间）
```

---

## 1.8 概率论基础：分类器的概率解释

### 1.8.1 从分类到概率

分类问题可以看作是对 $P(y|\mathbf{x})$ 建模——给定输入 $\mathbf{x}$，预测类别 $y$ 的概率。

::: info 数学知识
**条件概率（Conditional Probability）**

**定义：** 在事件 B 发生的条件下，事件 A 发生的概率。

**公式：**
$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

**直观理解：** "在已知 B 的情况下，A 发生的可能性有多大？"

**在 AI 中的应用：** $P(y|\mathbf{x})$ 表示给定输入 $\mathbf{x}$ 时类别为 $y$ 的概率。
:::

::: info 数学知识
**贝叶斯定理（Bayes' Theorem）**

**公式：**
$$
P(y|\mathbf{x}) = \frac{P(\mathbf{x}|y)P(y)}{P(\mathbf{x})}
$$

**各项含义：**
- $P(y|\mathbf{x})$：后验概率（给定数据 x 时类别 y 的概率）
- $P(\mathbf{x}|y)$：似然（类别 y 产生数据 x 的概率）
- $P(y)$：先验概率（类别 y 的基础概率）
- $P(\mathbf{x})$：证据（数据 x 出现的总概率）

**在 AI 中的应用：** 贝叶斯分类器的理论基础，解释为什么模型会预测某个类别。
:::

### 1.8.2 Softmax：将分数转换为概率

::: info 数学知识
**Softmax 函数**

**公式：**
$$
\text{Softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

其中 $\mathbf{z} = [z_1, z_2, ..., z_K]$ 是模型输出的 logits，$K$ 是类别数。

**特点：**
- 输出所有类别的概率和为 1
- 较大的 logit 对应较大的概率
- 温度参数可以控制分布的"锐利"程度

**在 AI 中的应用：** 多分类问题的标准输出层，将神经网络的原始输出转换为概率分布。
:::

**Softmax 示例：**

```python
import numpy as np

logits = np.array([2.0, 1.0, 0.1])  # 三个类别的原始输出

# Softmax 转换
exp_logits = np.exp(logits)
probabilities = exp_logits / np.sum(exp_logits)

print(f"Logits: {logits}")
print(f"Probabilities: {probabilities}")
print(f"Sum: {np.sum(probabilities)}")
```

输出：
```
Logits: [2.  1.  0.1]
Probabilities: [0.659  0.242  0.099]
Sum: 1.0
```

---

## 1.9 章节总结

### 核心概念回顾

| 概念 | 说明 |
|------|------|
| **分类问题** | 将输入映射到离散类别的监督学习问题 |
| **MNIST** | 手写数字数据集，28×28 灰度图像 |
| **KNN** | 最近邻分类器，基于距离的简单方法 |
| **维度灾难** | 高维空间中距离失去意义的现象 |
| **特征工程** | 人工设计特征表示 |
| **特征学习** | 从数据中自动学习特征表示 |
| **线性分类器** | 使用超平面进行分类的模型 |
| **感知机** | 最简单的线性分类器，学习算法 |
| **损失函数** | 衡量预测与真实标签差距的函数 |
| **梯度下降** | 通过负梯度方向迭代优化参数的算法 |
| **激活函数** | 引入非线性的变换函数 |

### 本章数学知识汇总

```
第 1 章数学知识地图：

┌─────────────────────────────────────────────────────────┐
│                    线性代数                              │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐              │
│  │  向量    │  │  矩阵    │  │  点积    │              │
│  └──────────┘  └──────────┘  └──────────┘              │
│  ┌──────────┐  ┌──────────┐                            │
│  │ L2范数   │  │ 线性变换 │                            │
│  └──────────┘  └──────────┘                            │
├─────────────────────────────────────────────────────────┤
│                    优化理论                              │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐              │
│  │  导数    │  │  梯度    │  │ 链式法则 │              │
│  └──────────┘  └──────────┘  └──────────┘              │
│  ┌──────────┐  ┌──────────┐                            │
│  │ MSE损失  │  │ 交叉熵   │                            │
│  └──────────┘  └──────────┘                            │
├─────────────────────────────────────────────────────────┤
│                    概率论                                │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐              │
│  │ 条件概率 │  │贝叶斯定理│  │ Softmax  │              │
│  └──────────┘  └──────────┘  └──────────┘              │
└─────────────────────────────────────────────────────────┘
```

### 章节要点

1. **识别数字是复杂的**：人类看似轻松的识别能力，其实涉及复杂的视觉处理过程

2. **KNN 简单但有限**：最近邻分类器直观易懂，但存在维度灾难和计算效率问题

3. **特征工程 vs 特征学习**：人工特征设计耗时且难以泛化，自动特征学习是更好的选择

4. **线性分类器有局限**：单层线性分类器无法解决 XOR 问题，需要多层网络

5. **学习就是优化**：神经网络通过梯度下降优化损失函数来学习参数

### 本章任务

::: important 本章任务清单
- [ ] 运行 KNN 代码，观察不同 k 值的效果
- [ ] 可视化 MNIST 数据，直观感受数字的多样性
- [ ] 实现感知机算法，理解其学习过程
- [ ] 手动计算梯度，理解梯度下降的原理
- [ ] 思考：如何解决线性分类器无法处理 XOR 的问题？
- [ ] 扩展阅读：了解感知机的历史和局限
- [ ] 编程实践：用 NumPy 实现一个简单的两层神经网络
:::

### 预告

下一章，我们将进入**第二世界线（视觉感知）**，学习卷积神经网络（CNN），看看如何通过局部连接和参数共享来解决图像处理的核心问题。

> *"单层的局限，需要多层来突破。"*
>
> —— 红莉栖

> *"从像素到边缘，从边缘到物体——机器学会了『看』。"*
>
> —— 冈部